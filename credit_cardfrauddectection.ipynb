{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5Nbq4BLjZmz",
        "outputId": "602e19f4-3157-43bd-8527-36b7a3a416a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "CREDIT CARD FRAUD DETECTION \n",
            "==============================\n",
            "============================================================\n",
            "STEP 1: DATA GENERATION (OPTIMIZED SMALLER SYNTHETIC DATA)\n",
            "============================================================\n",
            "✓ Dataset Shape: (20000, 31)\n",
            "✓ Legitimate: 19900 | Fraud: 100\n",
            "✓ Fraud Ratio: 0.500%\n",
            "\n",
            "============================================================\n",
            "STEP 2: PREPROCESSING\n",
            "============================================================\n",
            "✓ Train: (16000, 30), Test: (4000, 30)\n",
            "✓ After SMOTE: 31840 samples, Fraud Ratio: 50.0%\n",
            "\n",
            "============================================================\n",
            "STEP 3: TRAINING LIGHTWEIGHT MODELS\n",
            "============================================================\n",
            "→ Training Logistic Regression ...\n",
            "→ Training Random Forest ...\n",
            "→ Training Neural Network ...\n",
            "→ Training XGBoost ...\n",
            "✓ Models Trained Successfully!\n",
            "\n",
            "============================================================\n",
            "STEP 4: MODEL EVALUATION\n",
            "============================================================\n",
            "Logistic Regression: AUC=0.817, F1=0.039, Recall=0.800\n",
            "Random Forest: AUC=0.681, F1=0.019, Recall=0.150\n",
            "Neural Network: AUC=0.600, F1=0.023, Recall=0.050\n",
            "XGBoost: AUC=0.730, F1=0.026, Recall=0.250\n",
            "\n",
            "Final Comparison:\n",
            "                  Model  Accuracy  Precision  Recall  F1-Score   AUC-ROC\n",
            "0  Logistic Regression   0.80100   0.019802    0.80  0.038647  0.817161\n",
            "3              XGBoost   0.90675   0.013774    0.25  0.026110  0.729987\n",
            "1        Random Forest   0.92325   0.010239    0.15  0.019169  0.680578\n",
            "2       Neural Network   0.97850   0.014706    0.05  0.022727  0.600364\n",
            "\n",
            "✓ Completed in under ~15 seconds (depending on CPU speed)\n",
            "✓ Reduced sample size & tuned parameters for speed\n",
            "✓ SMOTE and lightweight models used efficiently\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Credit Card Fraud Detection (Optimized for Speed)\n",
        "Reduced Dataset, Lightweight Models, Fast SMOTE, and Early Exit\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, roc_auc_score, roc_curve, auc,\n",
        "    f1_score, precision_score, recall_score, accuracy_score\n",
        ")\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "    XGBOOST_AVAILABLE = True\n",
        "except ImportError:\n",
        "    XGBOOST_AVAILABLE = False\n",
        "    print(\"XGBoost not available. Install with: pip install xgboost\")\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# ============================================================================\n",
        "# 1. DATA CREATION (REDUCED SIZE)\n",
        "# ============================================================================\n",
        "def load_credit_card_data():\n",
        "    print(\"=\" * 60)\n",
        "    print(\"STEP 1: DATA GENERATION (OPTIMIZED SMALLER SYNTHETIC DATA)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    n_samples = 20000   # Reduced from 100,000 to 20,000\n",
        "    n_features = 28\n",
        "    fraud_ratio = 0.005  # 0.5% fraud\n",
        "\n",
        "    X_legit = np.random.randn(int(n_samples * (1 - fraud_ratio)), n_features)\n",
        "    y_legit = np.zeros(len(X_legit))\n",
        "    X_fraud = np.random.randn(int(n_samples * fraud_ratio), n_features) * 1.1 + 0.3\n",
        "    y_fraud = np.ones(len(X_fraud))\n",
        "\n",
        "    X = np.vstack([X_legit, X_fraud])\n",
        "    y = np.hstack([y_legit, y_fraud])\n",
        "    feature_names = [f'V{i}' for i in range(1, n_features + 1)]\n",
        "    df = pd.DataFrame(X, columns=feature_names)\n",
        "    df['Time'] = np.random.randint(0, 86400, len(df))\n",
        "    df['Amount'] = np.abs(np.random.exponential(100, len(df)))\n",
        "    df['Class'] = y\n",
        "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    print(f\"✓ Dataset Shape: {df.shape}\")\n",
        "    print(f\"✓ Legitimate: {(df['Class']==0).sum()} | Fraud: {(df['Class']==1).sum()}\")\n",
        "    print(f\"✓ Fraud Ratio: {100 * df['Class'].mean():.3f}%\")\n",
        "    return df\n",
        "\n",
        "# ============================================================================\n",
        "# 2. PREPROCESSING (SIMPLE SCALING + FAST SMOTE)\n",
        "# ============================================================================\n",
        "def preprocess_data(df):\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 2: PREPROCESSING\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    X = df.drop(\"Class\", axis=1)\n",
        "    y = df[\"Class\"]\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, stratify=y, random_state=42\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    # Faster SMOTE with fewer neighbors\n",
        "    smote = SMOTE(random_state=42, k_neighbors=2)\n",
        "    X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "    print(f\"✓ Train: {X_train.shape}, Test: {X_test.shape}\")\n",
        "    print(f\"✓ After SMOTE: {len(y_train_res)} samples, Fraud Ratio: {y_train_res.mean()*100:.1f}%\")\n",
        "    return X_train_res, X_test_scaled, y_train_res, y_test\n",
        "\n",
        "# ============================================================================\n",
        "# 3. MODEL TRAINING (LIGHTWEIGHT MODELS)\n",
        "# ============================================================================\n",
        "def train_models(X_train, y_train):\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 3: TRAINING LIGHTWEIGHT MODELS\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    models = {\n",
        "        \"Logistic Regression\": LogisticRegression(C=0.2, max_iter=500, random_state=42),\n",
        "        \"Random Forest\": RandomForestClassifier(\n",
        "            n_estimators=50, max_depth=6, min_samples_split=10,\n",
        "            min_samples_leaf=5, random_state=42\n",
        "        ),\n",
        "        \"Neural Network\": MLPClassifier(\n",
        "            hidden_layer_sizes=(16,), alpha=0.003, max_iter=100,\n",
        "            early_stopping=True, random_state=42\n",
        "        )\n",
        "    }\n",
        "\n",
        "    if XGBOOST_AVAILABLE:\n",
        "        models[\"XGBoost\"] = xgb.XGBClassifier(\n",
        "            n_estimators=50, max_depth=3, learning_rate=0.15,\n",
        "            subsample=0.8, colsample_bytree=0.8,\n",
        "            reg_lambda=1, eval_metric=\"logloss\", use_label_encoder=False, random_state=42\n",
        "        )\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"→ Training {name} ...\")\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "    print(\"✓ Models Trained Successfully!\")\n",
        "    return models\n",
        "\n",
        "# ============================================================================\n",
        "# 4. EVALUATION (FAST + CLEAN OUTPUT)\n",
        "# ============================================================================\n",
        "def evaluate_models(models, X_test, y_test):\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"STEP 4: MODEL EVALUATION\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    results = []\n",
        "    for name, model in models.items():\n",
        "        y_pred = model.predict(X_test)\n",
        "        y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        prec = precision_score(y_test, y_pred, zero_division=0)\n",
        "        rec = recall_score(y_test, y_pred)\n",
        "        f1 = f1_score(y_test, y_pred)\n",
        "        auc_ = roc_auc_score(y_test, y_proba)\n",
        "\n",
        "        results.append({\n",
        "            \"Model\": name, \"Accuracy\": acc, \"Precision\": prec,\n",
        "            \"Recall\": rec, \"F1-Score\": f1, \"AUC-ROC\": auc_\n",
        "        })\n",
        "\n",
        "        print(f\"{name}: AUC={auc_:.3f}, F1={f1:.3f}, Recall={rec:.3f}\")\n",
        "\n",
        "    results_df = pd.DataFrame(results).sort_values(by=\"AUC-ROC\", ascending=False)\n",
        "    print(\"\\nFinal Comparison:\\n\", results_df)\n",
        "    return results_df\n",
        "\n",
        "# ============================================================================\n",
        "# 5. MAIN\n",
        "# ============================================================================\n",
        "def main():\n",
        "    print(\"\\n\" + \"=\" * 30)\n",
        "    print(\"CREDIT CARD FRAUD DETECTION \")\n",
        "    print(\"=\" * 30)\n",
        "\n",
        "    df = load_credit_card_data()\n",
        "    X_train, X_test, y_train, y_test = preprocess_data(df)\n",
        "    models = train_models(X_train, y_train)\n",
        "    evaluate_models(models, X_test, y_test)\n",
        "\n",
        "    print(\"\\n✓ Completed in under ~15 seconds (depending on CPU speed)\")\n",
        "    print(\"✓ Reduced sample size & tuned parameters for speed\")\n",
        "    print(\"✓ SMOTE and lightweight models used efficiently\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}